# The local models you expect to be present
OLLAMA_MODEL=llama3.1:8b
OLLAMA_MODEL_JUDGE=qwen2.5:7b
OLLAMA_MODEL_EMBED=nomic-embed-text

# Optional: override host (e.g., WSL/remote)
# OLLAMA_HOST=http://127.0.0.1:11434